# TODO.md â€” Implementation & Improvement Checklist
## SentinelNet IDS â€” Intelligent Traffic Control and Intrusion Prevention Network

> Track implementation progress and improvements phase by phase.  
> Mark items with `[x]` when completed.  
> Last updated: **2026-02-10**

---

## Project Stats

| Component | Status | Key Metric |
|---|---|---|
| **ML Dataset** | âœ… Dual-Mode | Simple (98.5% F1) + Realistic (91.3% F1) |
| **ML Model (Simple)** | âœ… v3.0 | F1=0.9851, AUC=0.9991, FPR=1.0% |
| **ML Model (Realistic)** | âœ… v3.0 | F1=0.9131, AUC=0.9805, FPR=2.33% |
| **Ensemble** | âœ… IF+LOF | Weighted voting (65/35), adaptive to difficulty |
| **Rule IDS** | âœ… Enhanced | 6 detection rules, configurable thresholds |
| **Fusion Engine** | âœ… Enhanced | Weighted scoring, alert deduplication |
| **Backend API** | âœ… Enhanced | 12 endpoints, config/clear/stats |
| **Frontend** | âœ… Redesigned | NOC aesthetic, 4 pages, motion animations |
| **Visualizations** | âœ… Complete | 9 PNG plots per mode (18 total) |
| **Training Logs** | âœ… Complete | Separate logs for each mode |
| **Comparison** | âœ… Complete | COMPARISON.md - full analysis |
| **GitHub** | âœ… Pushed | All artifacts tracked (models, data, metrics, plots) |

---

## PHASE 1: Project Setup & Architecture âœ…
- [x] Create full folder structure as per ROADMAP
- [x] Initialize Python virtual environment
- [x] Create `requirements.txt` with all backend/ML dependencies
- [x] Initialize React app with Vite
- [x] Install frontend dependencies (axios, recharts, tailwindcss, motion, lucide-react)
- [x] Create `__init__.py` files for all Python packages
- [x] Define traffic feature schema (JSON)
- [x] Define alert severity levels: `LOW`, `MEDIUM`, `HIGH`, `CRITICAL`
- [x] GitHub repository configured and all code pushed
- [ ] Create architecture diagram (Mermaid in README)

---

## PHASE 2: Traffic Data Ingestion Module âœ…
- [x] Create `backend/main.py` â€” FastAPI app with CORS
- [x] Pydantic models with validation (IP, positive numbers, protocols)
- [x] `POST /traffic/ingest` endpoint
- [x] In-memory traffic store with max capacity
- [x] `GET /traffic/recent` â€” return last N records
- [x] Logging for ingested traffic
- [x] `connection_count` field added to traffic model

---

## PHASE 3: Rule-Based IDS âœ…
- [x] Configurable thresholds for port scan, flood, protocol anomaly
- [x] `detect_port_scan()`, `detect_flood()`, `detect_protocol_anomaly()`
- [x] `detect_syn_flood()` â€” TCP + high rate + tiny packets + short duration
- [x] `detect_slowloris()` â€” TCP + low rate + very long duration
- [x] `detect_dns_amplification()` â€” UDP + high rate + large packets
- [x] Combined `analyze()` returning highest severity alert + all_rules_triggered
- [x] Thresholds configurable via API (GET/PUT `/system/config`)
- [ ] Add brute force detection (many connections to same dst, same port)
- [ ] Add rate-based sliding window (track per-IP rates over time)
- [ ] Write unit tests for each detection rule

---

## PHASE 4: ML-Based Anomaly Detection âœ…

### Dataset (v3.0 â€” Anti-Overfitting)
- [x] **20,000 normal rows** across 7 traffic profiles
  - Web browsing (45%), Streaming (18%), DNS (12%), ICMP (5%)
  - **Edge cases (10%)** â€” borderline traffic that looks suspicious but is legitimate
  - IoT/Sensor (5%), Database/API (5%)
- [x] **8,000 attack rows** across 10 subtypes (800 each)
  - Port Scan, SYN Flood, UDP Flood, Slowloris, DNS Amplification
  - Protocol Anomaly, Brute Force, ICMP Flood, HTTP Flood, Stealthy Probe
- [x] **connection_count** feature added â€” differentiates brute force attacks
- [x] Datasets saved as CSV and tracked in git (`data/`)

### Training Pipeline (v3.0 â€” Ensemble + Visualizations)
- [x] **10 features**: 6 raw + 4 derived (`bytes_per_second`, `port_scan_ratio`, `size_rate_ratio`, `conn_rate`)
- [x] **StandardScaler** fitted on training data only (saved to `ml/scaler.pkl`)
- [x] **Proper train/validation/test split** â€” 70/15/15 (14K train, 3K val, 3K test)
- [x] **Hyperparameter grid search** â€” 81 combinations on validation set
  - Best: `n_estimators=100, contamination=0.01, max_features=0.75, max_samples=0.5`
- [x] **5-fold cross-validation** â€” FPR: 0.0108 Â± 0.0024
- [x] **Learning curve analysis** â€” F1 stable/improving, no overfitting (train-test gap=0.0)
- [x] **Feature importance** with confidence intervals (10 permutation repeats)
- [x] **Model comparison**: Isolation Forest vs One-Class SVM vs LOF vs Ensemble
- [x] **Ensemble model**: IF (65%) + LOF (35%) weighted voting
- [x] **Training logs** saved to `ml/training.log`

### Test Results (Held-Out Test Set â€” v3.0)
- [x] **F1 Score: 0.9851** | Precision: 0.9755 | Recall: 0.9950
- [x] **ROC-AUC: 0.9991** | FPR: 1.0%
- [x] **Per-attack detection rates** (ALL â‰¥ 96.6%):
  - Brute Force: **100%** âœ… (was 0% in v2.0, now fixed with connection_count + conn_rate)
  - Port Scan: 100% | ICMP Flood: 100% | DNS Amplification: 100%
  - SYN Flood: 100% | UDP Flood: 100% | Slowloris: 100%
  - Stealth Probe: 99.3% | HTTP Flood: 99.2% | Protocol Anomaly: 96.6%

### Visualizations (9 images in `ml/plots/`)
- [x] `confusion_matrix.png` â€” Primary model confusion matrix heatmap
- [x] `confusion_matrices_all.png` â€” All 4 models side-by-side
- [x] `roc_curves.png` â€” ROC curves for all models
- [x] `learning_curves.png` â€” Training vs validation accuracy + F1/loss curves
- [x] `per_attack_detection.png` â€” Per-attack detection rate bar chart
- [x] `feature_importance.png` â€” Feature importance with error bars
- [x] `score_distribution.png` â€” Normal vs attack anomaly score histograms
- [x] `model_comparison.png` â€” Grouped bar chart of all metrics
- [x] `cross_validation.png` â€” 5-fold CV FPR consistency

### Inference
- [x] Loads model + LOF + scaler from disk
- [x] Computes all 10 features (raw + derived) matching training pipeline
- [x] Ensemble voting (IF+LOF) with fallback to IF-only
- [x] Calibrated confidence scoring

### Artifacts (all tracked in git)
- [x] `ml/model.pkl` â€” trained Isolation Forest
- [x] `ml/ensemble_lof.pkl` â€” trained LOF for ensemble
- [x] `ml/scaler.pkl` â€” StandardScaler
- [x] `ml/training_metrics.json` â€” full metrics + hyperparams + learning curve
- [x] `ml/TRAINING_REPORT.md` â€” human-readable training report
- [x] `ml/training.log` â€” complete training log
- [x] `ml/plots/` â€” 9 visualization images
- [x] `data/normal_traffic.csv` â€” 20,000 normal traffic samples
- [x] `data/attack_traffic.csv` â€” 8,000 attack traffic samples

### Future Improvements
- [ ] Add temporal/sequential features (sliding window aggregation per-IP)
- [ ] Implement online learning / model drift detection
- [ ] Collect real-world traffic data for fine-tuning

---

## PHASE 5: Decision Fusion Engine âœ…
- [x] Severity mapping logic (Rule + ML combinations)
- [x] Structured `FusionDecision` output
- [x] Weighted scoring (configurable rule vs ML weight)
- [x] Alert deduplication (10-second sliding window per IP + attack_type)
- [x] Returns `duplicate` flag and `rules_matched` count
- [ ] Add historical pattern matching (repeated alerts from same IP)
- [ ] Write unit tests for all severity combinations

---

## PHASE 6: Policy Generation Engine âœ…
- [x] ACL generator with severity-based rules (BLOCK/RESTRICT/MONITOR)
- [x] Routing engine with OSPF cost recommendations
- [x] Cisco IOS-compatible ACL command output
- [ ] Add ACL rule conflict detection (duplicate/overlapping rules)
- [ ] Add policy expiration (auto-remove after configurable TTL)

---

## PHASE 7: Backend REST API Layer âœ…

### Endpoints Implemented
| Endpoint | Method | Description |
|---|---|---|
| `/traffic/ingest` | POST | Ingest traffic record â†’ IDS pipeline |
| `/traffic/recent` | GET | Recent traffic records |
| `/traffic/simulate` | POST | Simulate traffic (9 modes) |
| `/alerts/current` | GET | Current active alerts |
| `/alerts/history` | GET | Alert history with limit |
| `/policies/generated` | GET | Generated security policies |
| `/policies/latest` | GET | Latest policy |
| `/system/status` | GET | System health status |
| `/system/stats` | GET | Stats + attack breakdown + top offending IPs |
| `/system/config` | GET | IDS threshold configuration |
| `/system/config` | PUT | Update IDS thresholds at runtime |
| `/system/clear` | POST | Clear all in-memory stores (demo reset) |

### Future Improvements
- [ ] Add `/export/alerts` endpoint (CSV/JSON export)
- [ ] Add API error handling middleware (consistent error format)
- [ ] Add request logging middleware
- [ ] Add proper pagination (offset + limit + total)

---

## PHASE 8: React Dashboard âœ…

### Design System
- [x] NOC/command-center aesthetic (midnight blue + electric teal)
- [x] Outfit font (display) + JetBrains Mono (data/code)
- [x] CSS custom properties for theming
- [x] Custom SVG favicon (hexagon shield)
- [x] Motion (framer-motion) for animations

### Components
- [x] `Sidebar` â€” animated nav indicator with spring animation, shield logo
- [x] `Header` â€” live clock, connection status indicator, security mode badge
- [x] `KPICard` â€” 5 color variants, staggered entrance, radial gradient accent
- [x] `AlertTable` â€” expandable detail rows, confidence bars, severity coloring
- [x] `TrafficChart` â€” area chart with gradient fill, custom tooltip
- [x] `PolicyBlock` â€” severity gradient, two-column ACL/routing layout, copy button
- [x] `Panel` â€” shared wrapper with consistent styling + motion entrance

### Pages
- [x] `Dashboard` â€” KPIs, simulation buttons, security mode, severity/protocol/attack breakdowns, top IPs, recent alerts
- [x] `Traffic` â€” simulation control (9 modes dropdown), packet rate chart, protocol chart, traffic table with protocol badges
- [x] `Alerts` â€” severity summary cards (clickable filters), filter bar, expandable alert table, clear button
- [x] `Policies` â€” summary stats, active policies list, empty state

### Frontend API Service
- [x] All backend endpoints covered
- [x] Added: `getAlerts`, `getPolicies`, `clearSystem`, `getSystemConfig`, `updateSystemConfig`

### Future Improvements
- [ ] Dark/light mode toggle
- [ ] Loading skeletons (not just spinner)
- [ ] Error boundary components (graceful API failure handling)
- [ ] WebSocket for real-time updates (replace polling)
- [ ] Responsive mobile layout

---

## PHASE 9: Traffic Simulator âœ…
- [x] 7 specific attack generators: normal, port_scan, flood, syn_flood, slowloris, dns_amplification, anomaly
- [x] 2 meta modes: `random` (65% normal, 35% attacks), `mixed_attack` (uniform random)
- [x] Each generator uses distinct IP subnet ranges
- [x] `connection_count` included in all generators
- [x] Wired to `/traffic/simulate?mode=X&count=N`
- [ ] Add burst mode (rapid-fire many packets for DDoS simulation)
- [ ] Add auto-simulate mode (continuous background traffic for demo)

---

## PHASE 10: Cisco Packet Tracer Integration â³
- [ ] Design network topology in Packet Tracer
- [ ] Configure OSPF dynamic routing between routers
- [ ] Document integration steps with screenshots

---

## PHASE 11: Documentation & Presentation ðŸ“
- [ ] Write project report
- [ ] Create PowerPoint presentation
- [ ] Create README.md with full setup instructions
- [ ] Prepare viva Q&A document

---

## Version History

| Version | Date | Changes |
|---|---|---|
| v1.0 | 2026-02-09 | Initial implementation (5K data, 5 features, basic IDS) |
| v2.0 | 2026-02-10 | Anti-overfitting overhaul (20K+8K data, 8 features, grid search, per-attack eval) |
| v3.0 | 2026-02-10 | Ensemble (IF+LOF), 10 features, 9 visualizations, training logs, brute force fix (0%â†’100%) |

## Git Repository

**URL:** https://github.com/ArrinPaul/sentinelnet-ids

### What's Tracked
- All source code (backend, frontend, ML scripts)
- Trained ML models (`ml/model.pkl`, `ml/ensemble_lof.pkl`, `ml/scaler.pkl`)
- Training metrics (`ml/training_metrics.json`)
- Training report (`ml/TRAINING_REPORT.md`)
- Training log (`ml/training.log`)
- Visualization images (`ml/plots/*.png`)
- Datasets (`data/normal_traffic.csv`, `data/attack_traffic.csv`)

### What's Ignored
- `__pycache__/`, `.venv/`, `node_modules/`, `frontend/dist/`
- IDE files (`.vscode/`, `.idea/`)
- OS files (`.DS_Store`, `Thumbs.db`)

---

## Quick Start

```bash
# Backend
cd "CN Final Project"
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

# Generate dataset (choose difficulty)
python ml/generate_data.py          # Simple mode (default, ~98% F1)
python ml/generate_data.py realistic # Realistic mode (harder, ~91% F1)

# Train model (choose difficulty)
python ml/train_model.py            # Simple mode (~70 seconds)
python ml/train_model.py realistic  # Realistic mode (~105 seconds)

# Compare datasets
python ml/compare_datasets.py       # Side-by-side analysis

# Start backend
uvicorn backend.main:app --port 8000

# Frontend
cd frontend
npm install
npm run dev                         # http://localhost:5173
```

---

## v3.0 UPDATE: Dual-Mode Datasets (Feb 10, 2026)

### Background
Initial v3.0 achieved **98.51% F1 score**, which raised concerns about being "too good to be true." 
Data analysis revealed **91.5% of attacks** were easily detectable using a simple rule: `connection_count > 13`.

### Solution: Dual-Mode Datasets
Created **two difficulty modes** to balance learning demonstration vs real-world accuracy:

#### **Simple Mode** (Original)
- **Purpose:** Learning, proof of concept, demonstration
- **Characteristics:**
  - Normal connection_count: 1-19
  - Attack connection_count: mostly 50-500
  - 91.5% detectable by trivial threshold rule
- **Results:** F1=0.9851, AUC=0.9991, FPR=1.0%
- **All attacks:** â‰¥96.6% detection rate (brute force 100%)

#### **Realistic Mode** (New)
- **Purpose:** Production testing, real-world evaluation
- **Characteristics:**
  - Normal connection_count: 1-119 (added bulk operations, server tasks)
  - Attack connection_count: 1-499 (50% brute force now stealthy 8-50 range)
  - Only 68.4% easily detectable
  - 72.2% of attacks overlap with normal ranges
- **Results:** F1=0.9131, AUC=0.9805, FPR=2.33%
- **Challenging attacks:** Brute force 49.6%, Stealth probe 76.4%

### File Structure Changes
```
data/
â”œâ”€â”€ simple/          # Original dataset
â””â”€â”€ realistic/       # New challenging dataset

ml/results/
â”œâ”€â”€ simple/          # Simple mode training results
â”‚   â”œâ”€â”€ model.pkl, ensemble_lof.pkl, scaler.pkl
â”‚   â”œâ”€â”€ training_metrics.json, TRAINING_REPORT.md
â”‚   â””â”€â”€ plots/ (9 visualizations)
â””â”€â”€ realistic/       # Realistic mode training results
    â”œâ”€â”€ model.pkl, ensemble_lof.pkl, scaler.pkl
    â”œâ”€â”€ training_metrics.json, TRAINING_REPORT.md
    â””â”€â”€ plots/ (9 visualizations)

COMPARISON.md        # Comprehensive side-by-side analysis
```

### Key Improvements
- **Honest ML evaluation**: Both modes documented with transparent accuracy explanations
- **Flexible usage**: Choose mode based on use case (learning vs production)
- **Data generation script**: `python ml/generate_data.py [simple|realistic]`
- **Training script**: `python ml/train_model.py [simple|realistic]`
- **Comparison tool**: `python ml/compare_datasets.py` for analysis

### Verdict
- âœ“ **Simple mode accuracy (98.5%) is legitimate** - pipeline works correctly
- âœ“ **BUT data is too clean** - explains unrealistic performance
- âœ“ **Realistic mode (91.3%) is production-ready** - genuinely challenging
- âœ“ **Both modes have value** - learning vs real-world testing
